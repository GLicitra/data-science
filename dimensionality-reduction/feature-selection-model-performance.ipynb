{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Based on model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib   import Path\n",
    "from sklearn.preprocessing     import StandardScaler\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn.metrics           import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble          import RandomForestClassifier\n",
    "from sklearn.ensemble          import GradientBoostingRegressor\n",
    "from sklearn.ensemble          import RandomForestRegressor\n",
    "from sklearn.linear_model      import Lasso\n",
    "from sklearn.linear_model      import LassoCV\n",
    "from sklearn.linear_model      import LogisticRegression\n",
    "from sklearn.linear_model      import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Root directory = C:\\Users\\giann\\data-science-core\n"
     ]
    }
   ],
   "source": [
    "# set root directory\n",
    "path_root = Path(\"C:/Users/giann/data-science-core\")\n",
    "os.chdir(path_root)\n",
    "print(f'- Root directory = {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>family</th>\n",
       "      <th>age</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  diastolic  triceps  insulin   bmi  family  age      test\n",
       "0         1       89         66       23       94  28.1   0.167   21  negative\n",
       "1         0      137         40       35      168  43.1   2.288   33  positive\n",
       "2         3       78         50       32       88  31.0   0.248   26  positive\n",
       "3         2      197         70       45      543  30.5   0.158   53  positive\n",
       "4         1      189         60       23      846  30.1   0.398   59  positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/PimaIndians.csv'\n",
    "data  = pd.read_csv(path_dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('test', axis = 1)\n",
    "y = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.3, stratify = y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop features using Recursive Feature Elimination (RFE)\n",
    "- Dropping one feature at a time is **safest** since it will affect other feature's coef\n",
    "- RFE can be **wrapped** to any model that produces feature coefficient\n",
    "- RFE drop first the model and then drop the feature with the weakest coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "  n_features_to_select=3, step=1, verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator = LogisticRegression(solver = 'lbfgs'), n_features_to_select=3, verbose=1)\n",
    "# Fit the eliminator to the data\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 3, 'glucose': 1, 'diastolic': 6, 'triceps': 4, 'insulin': 5, 'bmi': 1, 'family': 2, 'age': 1}\n"
     ]
    }
   ],
   "source": [
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values of 1 on `rfe.rarking` means that the feature was kept till the end, and viceversa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'bmi', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree-based feature selection\n",
    "- Radom forest does not requirte feature scaling.\n",
    "- it is important also in this case tuo use `RFE` since dropping one feature might give more weight to another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>diastolic</th>\n",
       "      <th>triceps</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>family</th>\n",
       "      <th>age</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  diastolic  triceps  insulin   bmi  family  age      test\n",
       "0         1       89         66       23       94  28.1   0.167   21  negative\n",
       "1         0      137         40       35      168  43.1   2.288   33  positive\n",
       "2         3       78         50       32       88  31.0   0.248   26  positive\n",
       "3         2      197         70       45      543  30.5   0.158   53  positive\n",
       "4         1      189         60       23      846  30.1   0.398   59  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/PimaIndians.csv'\n",
    "data  = pd.read_csv(path_dataset)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('test', axis = 1)\n",
    "y = data['test']\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state = 0, n_estimators = 10)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pregnant': 0.09, 'glucose': 0.21, 'diastolic': 0.08, 'triceps': 0.11, 'insulin': 0.13, 'bmi': 0.09, 'family': 0.12, 'age': 0.16}\n"
     ]
    }
   ],
   "source": [
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.6% accuracy on test set.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the test set accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model gets 78% accuracy on the test set and `glucose` is the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for features importances above the threshold\n",
    "mask = rf.feature_importances_ > 0.15\n",
    "# Apply the mask to the feature dataset X\n",
    "reduced_X = X.loc[:, mask]\n",
    "# prints out the selected column names\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this way we might **discard** important feature. The **right way** is using the `RFE` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 4 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "  n_features_to_select=2, step=2, verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap the feature eliminator around the random forest model\n",
    "rfe = RFE(estimator = RandomForestClassifier(random_state = 0, n_estimators = 10), n_features_to_select = 2, verbose = 1, step = 2)\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['glucose', 'insulin'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case the feature have been **changed**!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction for regressor problems via Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/ANSUR_II_FEMALE.csv'\n",
    "data  = pd.read_csv(path_dataset)\n",
    "# exclude categorical variable\n",
    "data = data.select_dtypes(exclude = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = data.drop('BMI', axis = 1)\n",
    "y = data['BMI']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=3000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Lasso model\n",
    "la = Lasso(max_iter = 3000)\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o The model can predict 80.5% of the variance in the test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance\n",
    "# Scale the test set\n",
    "X_test_std = scaler.transform(X_test)\n",
    "# Calculate the coefficient of determination (R squared) on the scaled test set\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"o The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o The model has ignored 86 out of 93 features.\n"
     ]
    }
   ],
   "source": [
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "# Take the sum of this list\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"o The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to tune $alpha$ to improve the performance. For this reason it is best to use the `LassoCV` class to automatically tune the best alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LassoCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/ANSUR_II_FEMALE.csv'\n",
    "data  = pd.read_csv(path_dataset)\n",
    "# exclude categorical variable\n",
    "data = data.select_dtypes(exclude = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = data.drop('BMI', axis = 1)\n",
    "y = data['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Scale and split\n",
    "scaler = StandardScaler()\n",
    "X_std  = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o Optimal alpha = 0.003\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV(cv = 5)\n",
    "lcv.fit(X_train, y_train)\n",
    "print('o Optimal alpha = {0:.3f}'.format(lcv.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o The model explains 99.5% of the test set variance\n"
     ]
    }
   ],
   "source": [
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('o The model explains {0:.1%} of the test set variance'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o 54 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('o {} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models for extra votes\n",
    "One can combine multiple models and carry out feature selection using a **metamodel**. Here we use **Lasso Regression**, **Random Forest** and **Gradient Boosting** to perform feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "path_dataset = path_root / 'dataset/ANSUR_II_FEMALE.csv'\n",
    "data  = pd.read_csv(path_dataset)\n",
    "# exclude categorical variable\n",
    "data = data.select_dtypes(exclude = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = data.drop('BMI', axis = 1)\n",
    "y = data['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "# Scale and split\n",
    "scaler = StandardScaler()\n",
    "X_std  = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1**: Let us train a model one by one;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "    positive=False, precompute='auto', random_state=None,\n",
       "    selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcv = LassoCV(cv = 5)\n",
    "lcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o The model explains 99.0% of the test set variance\n"
     ]
    }
   ],
   "source": [
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('o The model explains {0:.1%} of the test set variance'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o 40 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('o {} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))\n",
    "n_feature_selected = sum(lcv_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting**: use `RFE` and select the same number of features obtained with `LassoCV`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 43 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_sampl...=None, subsample=1.0, tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=0, warm_start=False),\n",
       "  n_features_to_select=40, step=5, verbose=1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
    "rfe_gb = RFE(estimator = GradientBoostingRegressor(n_estimators = 10), n_features_to_select = n_feature_selected, step = 5, verbose = 1)\n",
    "rfe_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 75.2% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_gb.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o 40 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "# Assign the support array to gb_mask\n",
    "gb_mask = rfe_gb.support_\n",
    "print('o {} features out of {} selected'.format(sum(gb_mask), len(gb_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**: use `RFE` and select the same number of features obtained with `LassoCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 93 features.\n",
      "Fitting estimator with 88 features.\n",
      "Fitting estimator with 83 features.\n",
      "Fitting estimator with 78 features.\n",
      "Fitting estimator with 73 features.\n",
      "Fitting estimator with 68 features.\n",
      "Fitting estimator with 63 features.\n",
      "Fitting estimator with 58 features.\n",
      "Fitting estimator with 53 features.\n",
      "Fitting estimator with 48 features.\n",
      "Fitting estimator with 43 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "  n_features_to_select=40, step=5, verbose=1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator = RandomForestRegressor(n_estimators = 10), \n",
    "             n_features_to_select = n_feature_selected, step = 5, verbose = 1)\n",
    "rfe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 95.7% of the variance in the test set\n"
     ]
    }
   ],
   "source": [
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o 40 features out of 93 selected\n"
     ]
    }
   ],
   "source": [
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_\n",
    "print('o {} features out of {} selected'.format(sum(rf_mask), len(rf_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2**: **Mean voting** assess the votes of each model. The feature selection depend on how conservative one want to be. In this case we select the features that have been selected by all the three models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 1 3 3 1 1 1 3 1 3 1 2 1 1 3 3 2 0 1 3 2 1 3 2 2 2 1 1 0 0 0 1 1 0 0 0\n",
      " 2 2 0 1 0 0 0 0 1 0 0 0 0 1 2 1 2 0 1 1 0 0 3 0 2 0 0 0 3 0 1 1 1 1 1 1 2\n",
      " 3 3 3 3 0 1 1 2 1 2 1 3 3 1 2 0 2 3 2]\n"
     ]
    }
   ],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "print(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True,  True, False, False, False,  True,\n",
       "       False,  True, False, False, False, False,  True,  True, False,\n",
       "       False, False,  True, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False,  True,  True,  True,  True, False, False, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False,  True, False])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "meta_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3**: Apply the dimensionality reduction on `X` and fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X = data.drop('BMI', axis = 1)\n",
    "y = data['BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X.loc[:, meta_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model can explain 98.6% of the variance in the test set using 17 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giann\\Anaconda3\\envs\\data-science\\lib\\site-packages\\ipykernel_launcher.py:1: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
