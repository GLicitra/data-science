{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link: https://www.kaggle.com/dgawlik/nyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "data = pd.read_csv('prices.csv', index_col=0)\n",
    "\n",
    "# Convert the index of the DataFrame to datetime\n",
    "data.index = pd.to_datetime(data.index)\n",
    "print(data.head())\n",
    "\n",
    "# Loop through each column, plot its values over time\n",
    "fig, ax = plt.subplots()\n",
    "for column in data:\n",
    "    data[column].plot(ax=ax, label=column)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each company's value is sometimes correlated with others, and sometimes not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the dataset\n",
    "As mentioned in the video, you'll deal with stock market prices that fluctuate over time. In this exercise you've got historical prices from two tech companies (`Ebay` and `Yahoo`) in the DataFrame `prices`. You'll visualize the raw data for the two companies, then generate a scatter plot showing how the values for each company compare with one another. Finally, you'll add in a \"time\" dimension to your scatter plot so you can see how this relationship changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw values over time\n",
    "prices.plot()\n",
    "plt.show()\n",
    "    \n",
    "# Scatterplot with one company per axis\n",
    "prices.plot.scatter('EBAY', 'YHOO')\n",
    "plt.show()\n",
    "\n",
    "# Finally, encode time as the color of each datapoint in order to visualize how the relationship between \n",
    "# these two variables changes.\n",
    "# Scatterplot with color relating to time\n",
    "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, cmap=plt.cm.viridis, colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these two time series seem somewhat related to each other, though its a complex relationship that changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a simple regression model\n",
    "Now we'll look at a larger number of companies. Recall that we have historical price values for many companies. Let's use data from several companies to predict the value of a test company. You'll attempt to predict the value of the Apple stock price using the values of NVidia, Ebay, and Yahoo. Each of these is stored as a column in the all_prices DataFrame. Below is a mapping from company name to column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, shuffle=False, random_state=1)\n",
    "\n",
    "# Fit our model and generate predictions\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "score = r2_score(y_test, predictions)\n",
    "print(score)\n",
    "\n",
    "# Visualize our predictions along with the \"true\" values, and print the score\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.plot(y_test, color='k', lw=3)\n",
    "ax.plot(predictions, color='r', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have an explanation for your poor score. The predictions clearly deviate from the true time series values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "prices.plot(legend=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count the missing values of each time series\n",
    "missing_values = prices.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function we'll use to interpolate and plot\n",
    "def interpolate_and_plot(prices, interpolation):\n",
    "\n",
    "    # Create a boolean mask for missing values\n",
    "    missing_values = prices.isna()\n",
    "\n",
    "    # Interpolate the missing values\n",
    "    prices_interp = prices.interpolate(interpolation)\n",
    "\n",
    "    # Plot the results, highlighting the interpolated values in black\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
    "    \n",
    "    # Now plot the interpolated values on top in red\n",
    "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
    "    plt.show()\n",
    "    \n",
    "# Interpolate using the latest non-missing value\n",
    "interpolation_type = 'zero'\n",
    "interpolate_and_plot(prices, interpolation_type)\n",
    "\n",
    "# Interpolate linearly\n",
    "interpolation_type = 'linear'\n",
    "interpolate_and_plot(prices, interpolation_type)\n",
    "\n",
    "# Interpolate with a quadratic function\n",
    "interpolation_type = 'quadratic'\n",
    "interpolate_and_plot(prices, interpolation_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming raw data\n",
    "to help detecting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom function\n",
    "def percent_change(series):\n",
    "    # Collect all *but* the last value of this window, then the final value\n",
    "    previous_values = series[:-1]\n",
    "    last_value = series[-1]\n",
    "\n",
    "    # Calculate the % difference between the last value and the mean of earlier values\n",
    "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
    "    return percent_change\n",
    "\n",
    "# Apply your custom function and plot\n",
    "prices_perc = prices.rolling(20).apply(percent_change)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers\n",
    "In this exercise, you'll handle outliers - data points that are so different from the rest of your data, that you treat them differently from other \"normal-looking\" data points. You'll use the output from the previous exercise (percent change over time) to detect the outliers. First you will write a function that replaces outlier data points with the median value from the entire time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_outliers(series):\n",
    "    # Calculate the absolute difference of each timepoint from the series mean\n",
    "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
    "\n",
    "    # Calculate a mask for the differences that are > 3 standard deviations from the mean\n",
    "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
    "    \n",
    "    # Replace these values with the median accross the data\n",
    "    series[this_mask] = np.nanmedian(series)\n",
    "    return series\n",
    "\n",
    "# Apply your preprocessing function to the timeseries and plot the results\n",
    "prices_perc = prices_perc.apply(replace_outliers)\n",
    "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering multiple rolling features at once\n",
    "You'll calculate a collection of features for your time series data and visualize what they look like over time. This process resembles how many other time series models operate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "\n",
    "# Define the features you'll calculate for each window\n",
    "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
    "\n",
    "# Calculate these features for your rolling window object\n",
    "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
    "\n",
    "# Plot the results\n",
    "ax = features.loc[:\"2011-01\"].plot()\n",
    "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
    "ax.legend(loc=(1.01, .6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentiles and partial functions\n",
    "In this exercise, you'll practice how to pre-choose arguments of a function so that you can pre-configure how it runs. You'll use this to calculate several percentiles of your data using the same percentile() function in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import partial from functools\n",
    "from functools import partial\n",
    "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
    "\n",
    "# Use a list comprehension to create a partial function for each quantile\n",
    "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
    "\n",
    "# Calculate each of these quantiles on the data using a rolling window\n",
    "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
    "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
    "\n",
    "# Plot a subset of the result\n",
    "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
    "ax.legend(percentiles, loc=(1.01, .5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \"date\" information\n",
    "It's easy to think of timestamps as pure numbers, but don't forget they generally correspond to things that happen in the real world. That means there's often extra information encoded in the data such as \"is it a weekday?\" or \"is it a holiday?\". This information is often useful in predicting timeseries data. In this exercise, you'll extract these date/time based features. A single time series has been loaded in a variable called prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date features from the data, add them as columns\n",
    "prices_perc['day_of_week'] = prices_perc.index.dayofweek\n",
    "prices_perc['week_of_year'] = prices_perc.index.weekofyear\n",
    "prices_perc['month_of_year'] = prices_perc.index.month\n",
    "\n",
    "# Print prices_perc\n",
    "print(prices_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating time-shifted features\n",
    "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
    "\n",
    "In this exercise, you'll \"shift\" your raw data and visualize the results. You'll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the \"time lags\"\n",
    "shifts = np.arange(1, 11).astype(int)\n",
    "\n",
    "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
    "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
    "\n",
    "# Convert into a DataFrame for subsequent use\n",
    "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
    "\n",
    "# Plot the first 100 samples of each\n",
    "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
    "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the median for each column\n",
    "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
    "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
    "\n",
    "# Fit the model\n",
    "model = Ridge()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(coefs, names, ax):\n",
    "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
    "    ax.bar(names, coefs)\n",
    "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
    "    \n",
    "    # Set formatting so it looks nice\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "    return ax\n",
    "\n",
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-regression with a smoother time series\n",
    "Now, let's re-run the same procedure using a smoother signal. You'll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model?\n",
    "\n",
    "prices_perc_shifted and model (updated to use a window of 40) are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output data up to \"2011-01\"\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "y.loc[:'2011-01'].plot(ax=axs[0])\n",
    "\n",
    "# Run the function to visualize model's coefficients\n",
    "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K fold validation without shuffeling\n",
    "This time, the predictions generated within each CV loop look 'smoother' than they were before - they look more like a real time series because you didn't shuffle the data. This is a good sanity check to make sure your CV splits are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KFold cross-validation object\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=10, shuffle=False, random_state=1)\n",
    "\n",
    "# Iterate through CV splits\n",
    "results = []\n",
    "for tr, tt in cv.split(X, y):\n",
    "    # Fit the model on training data\n",
    "    model.fit(X[tr], y[tr])\n",
    "    \n",
    "    # Generate predictions on the test data and collect\n",
    "    prediction = model.predict(X[tt])\n",
    "    results.append((prediction, tt))\n",
    "    \n",
    "# Custom function to quickly visualize predictions\n",
    "visualize_predictions(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(tr, ii + y[tr])\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping a confidence interval\n",
    "A useful tool for assessing the variability of some data is the bootstrap. In this exercise, you'll write your own bootstrapping function that can be used to return a bootstrapped confidence interval.\n",
    "\n",
    "This function takes three parameters: a 2-D array of numbers (data), a list of percentiles to calculate (percentiles), and the number of boostrap iterations to use (n_boots). It uses the resample function to generate a bootstrap sample, and then repeats this many times to calculate the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_interval(data, percentiles=(2.5, 97.5), n_boots=100):\n",
    "    \"\"\"Bootstrap a confidence interval for the mean of columns of a 2-D dataset.\"\"\"\n",
    "    # Create our empty array to fill the results\n",
    "    bootstrap_means = np.zeros([n_boots, data.shape[-1]])\n",
    "    for ii in range(n_boots):\n",
    "        # Generate random indices for our data *with* replacement, then take the sample mean\n",
    "        random_sample = resample(data)\n",
    "        bootstrap_means[ii] = random_sample.mean(axis=0)\n",
    "        \n",
    "    # Compute the percentiles of choice for the bootstrapped means\n",
    "    percentiles = np.percentile(bootstrap_means, percentiles, axis=0)\n",
    "    return percentiles\n",
    "\n",
    "# Iterate through CV splits\n",
    "n_splits = 100\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Create empty array to collect coefficients\n",
    "coefficients = np.zeros([n_splits, X.shape[1]])\n",
    "\n",
    "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "    model.fit(X[tr], y[tr])\n",
    "    coefficients[ii] = model.coef_\n",
    "    \n",
    "# Calculate a confidence interval around each coefficient\n",
    "bootstrapped_interval = bootstrap_interval(coefficients)\n",
    "\n",
    "# Plot it\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(feature_names, bootstrapped_interval[0], marker='_', lw=3)\n",
    "ax.scatter(feature_names, bootstrapped_interval[1], marker='_', lw=3)\n",
    "ax.set(title='95% confidence interval for model coefficients')\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing model score variability over time\n",
    "Now that you've assessed the variability of each coefficient, let's do the same for the performance (scores) of the model. Recall that the TimeSeriesSplit object will use successively-later indices for each test set. This means that you can treat the scores of your validation as a time series. You can visualize this over time in order to see how the model's performance changes over time.\n",
    "\n",
    "An instance of the Linear regression model object is stored in model, a cross-validation object in cv, and data in X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "\n",
    "# Convert to a Pandas Series object\n",
    "scores_series = pd.Series(scores, index=times_scores, name='score')\n",
    "\n",
    "# Bootstrap a rolling confidence interval for the mean score\n",
    "scores_lo = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=2.5))\n",
    "scores_hi = scores_series.rolling(20).aggregate(partial(bootstrap_interval, percentiles=97.5))\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots()\n",
    "scores_lo.plot(ax=ax, label=\"Lower confidence interval\")\n",
    "scores_hi.plot(ax=ax, label=\"Upper confidence interval\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# You plotted a rolling confidence interval for scores over time. This is useful in seeing when your model predictions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounting for non-stationarity\n",
    "In this exercise, you will again visualize the variations in model scores, but now for data that changes its statistics over time.\n",
    "\n",
    "An instance of the Linear regression model object is stored in model, a cross-validation object in cv, and the data in X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-initialize window sizes\n",
    "window_sizes = [25, 50, 75, 100]\n",
    "\n",
    "# Create an empty DataFrame to collect the stores\n",
    "all_scores = pd.DataFrame(index=times_scores)\n",
    "\n",
    "# Generate scores for each split to see how the model performs over time\n",
    "for window in window_sizes:\n",
    "    # Create cross-validation object using a limited lookback window\n",
    "    cv = TimeSeriesSplit(n_splits=100, max_train_size=window)\n",
    "    \n",
    "    # Calculate scores across all CV splits and collect them in a DataFrame\n",
    "    this_scores = cross_val_score(model, X, y, cv=cv, scoring=my_pearsonr)\n",
    "    all_scores['Length {}'.format(window)] = this_scores\n",
    "    \n",
    "# Visualize the scores\n",
    "ax = all_scores.rolling(10).mean().plot(cmap=plt.cm.coolwarm)\n",
    "ax.set(title='Scores for multiple windows', ylabel='Correlation (r)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful - notice how in some stretches of time, longer windows perform worse than shorter ones. This is because the statistics in the data have changed, and the longer window is now using outdated information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsfresh\n",
    "tsfresh is a python package. It automatically calculates a large number of time series characteristics, the so called **features**. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for finance\n",
    "\n",
    "### Create train and test features\n",
    "Before we fit our linear model, we want to add a constant to our features, so we have an intercept for our linear model.\n",
    "\n",
    "We also want to create train and test features. This is so we can fit our model to the train dataset, and evaluate performance on the test dataset. We always want to check performance on data the model has not seen to make sure we're not overfitting, which is memorizing patterns in the training data too exactly.\n",
    "\n",
    "With a time series like this, we typically want to use the oldest data as our training set, and the newest data as our test set. This is so we can evaluate the performance of the model on the most recent data, which will more realistically simulate predictions on data we haven't seen yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the statsmodels library with the alias sm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant to the features\n",
    "linear_features = sm.add_constant(features)\n",
    "\n",
    "# Create a size for the training set that is 85% of the total number of samples\n",
    "train_size = int(0.85 * features.shape[0])\n",
    "train_features = linear_features[:train_size]\n",
    "train_targets = targets[:train_size]\n",
    "test_features = linear_features[train_size:]\n",
    "test_targets = targets[train_size:]\n",
    "print(linear_features.shape, train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear model\n",
    "We'll now fit a linear model, because they are simple and easy to understand. Once we've fit our model, we can see which predictor variables appear to be meaningfully linearly correlated with the target, as well as their magnitude of effect on the target. Our judgment of whether or not predictors are significant is based on the **p-values** of coefficients. This is using a t-test to statistically test if the coefficient significantly differs from 0. The p-value is the percent chance that the coefficient for a feature does not differ from zero. Typically, we take a p-value of less than **0.05** to mean the coefficient is significantly different from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear model and complete the least squares fit\n",
    "model = sm.OLS(train_targets, train_features)\n",
    "results = model.fit()  # fit the model\n",
    "print(results.summary())\n",
    "\n",
    "# examine pvalues\n",
    "# Features with p <= 0.05 are typically considered significantly different from 0\n",
    "print(results.pvalues)\n",
    "\n",
    "# Make predictions from our model for train and test sets\n",
    "train_predictions = results.predict(train_features)\n",
    "test_predictions  = results.predict(test_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our results\n",
    "Once we have our linear fit and predictions, we want to see how good the predictions are so we can decide if our model is any good or not. Ideally, we want to back-test any type of trading strategy. However, this is a complex and typically time-consuming experience.\n",
    "\n",
    "A quicker way to understand the performance of our model is looking at regression evaluation metrics like R2, and plotting the predictions versus the actual values of the targets. Perfect predictions would form a straight, diagonal line in such a plot, making it easy for us to eyeball how our predictions are doing in different regions of price changes. We can use matplotlib's .scatter() function to create scatter plots of the predictions and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter the predictions vs the targets with 80% transparency\n",
    "plt.scatter(train_predictions, train_targets, alpha=0.2, color='b', label='train')\n",
    "plt.scatter(test_predictions, test_targets, alpha = 0.2, color='r', label='test')\n",
    "\n",
    "# Plot the perfect prediction line\n",
    "xmin, xmax = plt.xlim()\n",
    "plt.plot(np.arange(xmin, xmax, 0.01), np.arange(xmin, xmax, 0.01), c='k')\n",
    "\n",
    "# Set the axis labels and show the plot\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('actual')\n",
    "plt.legend() # show the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering from volume\n",
    "We're going to use non-linear models to make more accurate predictions. With linear models, features must be linearly correlated to the target. Other machine learning models can combine features in non-linear ways. For example, what if the price goes up when the moving average of price is going up, and the moving average of volume is going down? The only way to capture those interactions is to either multiply the features, or to use a machine learning algorithm that can handle non-linearity (e.g. random forests).\n",
    "\n",
    "To incorporate more information that may interact with other features, we can add in weakly-correlated features. First we will add volume data, which we have in the lng_df as the Adj_Volume column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new volume features, 1-day % change and 5-day SMA of the % change\n",
    "new_features = ['Adj_Volume_1d_change', 'Adj_Volume_1d_change_SMA']\n",
    "feature_names.extend(new_features)\n",
    "lng_df['Adj_Volume_1d_change'] = lng_df['Adj_Volume'].pct_change()\n",
    "lng_df['Adj_Volume_1d_change_SMA'] = talib.SMA(lng_df['Adj_Volume_1d_change'].values,\n",
    "                                               timeperiod=5)\n",
    "\n",
    "# Plot histogram of volume % change data\n",
    "lng_df[new_features].plot(kind='hist', sharex=False, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create day-of-week features\n",
    "We can engineer datetime features to add even more information for our non-linear models. Most financial data has datetimes, which have lots of information in them -- year, month, day, and sometimes hour, minute, and second. But we can also get the day of the week, and things like the quarter of the year, or the elapsed time since some event (e.g. earnings reports).\n",
    "\n",
    "We are only going to get the day of the week here, since our dataset doesn't go back very far in time. The dayofweek property from the pandas datetime index will help us get the day of the week. Then we will dummy dayofweek with pandas' get_dummies(). This creates columns for each day of the week with binary values (0 or 1). We drop the first column because it can be inferred from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas' get_dummies function to get dummies for day of the week\n",
    "days_of_week = pd.get_dummies(lng_df.index.dayofweek,\n",
    "                              prefix='weekday',\n",
    "                              drop_first=True)\n",
    "\n",
    "# Set the index as the original DataFrame index for merging\n",
    "days_of_week.index = lng_df.index\n",
    "\n",
    "# Join the dataframe with the days of week DataFrame\n",
    "lng_df = pd.concat([lng_df, days_of_week], axis=1)\n",
    "\n",
    "# Add days of week to feature names\n",
    "feature_names.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "lng_df.dropna(inplace=True)  # drop missing values in-place\n",
    "print(lng_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though the correlations are weak, they may improve our predictions via interactions with other features.\n",
    "\n",
    "# Add the weekday labels to the new_features list\n",
    "new_features.extend(['weekday_' + str(i) for i in range(1, 5)])\n",
    "\n",
    "# Plot the correlations between the new features and the targets\n",
    "sns.heatmap(lng_df[new_features + ['5d_close_future_pct']].corr(), annot=True)\n",
    "plt.yticks(rotation=0)  # ensure y-axis ticklabels are horizontal\n",
    "plt.xticks(rotation=90)  # ensure x-axis ticklabels are vertical\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for Modern Portfolio Theory\n",
    "Our first step towards calculating modern portfolio theory (MPT) portfolios is to get daily and monthly returns. Eventually we're going to get the best portfolios of each month based on the Sharpe ratio. The easiest way to do this is to put all our stock prices into one DataFrame, then to resample them to the daily and monthly time frames. We need daily price changes to calculate volatility, which we will use as our measure of risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join 3 stock DataFrame together\n",
    "full_df = pd.concat([lng_df, spy_df, smlv_df], axis=1).dropna()\n",
    "\n",
    "# Resample the full DataFrame to monthly timeframe\n",
    "# 'BMS' stands for Business Month Start\n",
    "monthly_df = full_df.resample('BMS').first()\n",
    "\n",
    "# Calculate daily returns of stocks\n",
    "returns_daily = full_df.pct_change()\n",
    "\n",
    "# Calculate monthly returns of the stocks\n",
    "returns_monthly = monthly_df.pct_change().dropna()\n",
    "print(returns_monthly.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate covariances for volatility\n",
    "In MPT, we quantify risk via volatility. The math for calculating portfolio volatility is complex, and it requires daily returns covariances. We'll now loop through each month in the `returns_monthly` DataFrame, and calculate the covariance of the daily returns.\n",
    "\n",
    "With pandas datetime indices, we can access the month and year with `df.index.month` and `df.index.year`. We'll use this to create a mask for returns_daily that gives us the daily returns for the current month and year in the loop. We then use the mask to subset the DataFrame like this: `df[mask]`. This gets entries in the returns_daily DataFrame which are in the current month and year in each cycle of the loop. Finally, we'll use pandas' .cov() method to get the covariance of daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily covariance of stocks (for each monthly period)\n",
    "covariances = {}\n",
    "rtd_idx = returns_daily.index\n",
    "for i in returns_monthly.index:    \n",
    "    # Mask daily returns for each month and year, and calculate covariance\n",
    "    mask = (rtd_idx.month == i.month) & (rtd_idx.year == i.year)\n",
    "    \n",
    "    # Use the mask to get daily returns for the current month and year of monthy returns index\n",
    "    covariances[i] = returns_daily[mask].cov()\n",
    "\n",
    "print(covariances[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate portfolios\n",
    "We'll now generate portfolios to find each month's best one. numpy's random.random() generates random numbers from a uniform distribution, then we normalize them so they sum to 1 using the /= operator. We use these weights to calculate returns and volatility. Returns are sums of weights times individual returns. Volatility is more complex, and involves the covariances of the different stocks.\n",
    "\n",
    "Finally we'll store the values in dictionaries for later use, with months' dates as keys.\n",
    "\n",
    "In this case, we will only generate 10 portfolios for each date so the code will run faster, but in a real-world use-case you'd want to use more like 1000 to 5000 randomly-generated portfolios for a few stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_returns, portfolio_volatility, portfolio_weights = {}, {}, {}\n",
    "\n",
    "# Get portfolio performances at each month\n",
    "for date in sorted(covariances.keys()):\n",
    "    cov = covariances[date]\n",
    "    for portfolio in range(10):\n",
    "        weights = np.random.random(3)\n",
    "        weights /= np.sum(weights) # /= divides weights by their sum to normalize \n",
    "        returns = np.dot(weights, returns_monthly.loc[date])\n",
    "        volatility = np.sqrt(np.dot(weights.T, np.dot(cov, weights)))\n",
    "        portfolio_returns.setdefault(date, []).append(returns)\n",
    "        portfolio_volatility.setdefault(date, []).append(volatility)\n",
    "        portfolio_weights.setdefault(date, []).append(weights)\n",
    "\n",
    "print(portfolio_weights[date][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot efficient frontier\n",
    "We can finally plot the results of our MPT portfolios, which shows the \"efficient frontier\". This is a plot of the volatility vs the returns. This can help us visualize our risk-return possibilities for portfolios. The upper left boundary of the points is the best we can do (highest return for a given risk), and that is the efficient frontier.\n",
    "\n",
    "To create this plot, we will use the latest date in our covariances dictionary which we created a few exercises ago. This has dates as keys, so we'll get the sorted keys using sorted() and .keys(), then get the last entry with Python indexing ([-1]). Lastly we'll use matplotlib to scatter variance vs returns and see the efficient frontier for the latest date in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest date of available data\n",
    "date = sorted(covariances.keys())[-1]  \n",
    "\n",
    "# Plot efficient frontier\n",
    "# warning: this can take at least 10s for the plot to execute...\n",
    "plt.scatter(x=portfolio_volatility[date], y=portfolio_returns[date], alpha=0.1)\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best Sharpe ratios\n",
    "We need to find the \"ideal\" portfolios for each date so we can use them as targets for machine learning. We'll loop through each date in portfolio_returns, then loop through the portfolios we generated with `portfolio_returns[date]`. We'll then calculate the Sharpe ratio, which is the return divided by volatility (assuming a no-risk return of 0).\n",
    "\n",
    "We use enumerate() to loop through the returns for the current date (`portfolio_returns[date]`) and keep track of the index with i. Then we use the current date and current index to get the volatility of each portfolio with `portfolio_volatility[date][i]`. Finally, we get the index of the best Sharpe ratio for each date using `np.argmax()`. We'll use this index to get the ideal portfolio weights soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionaries for sharpe ratios and best sharpe indexes by date\n",
    "sharpe_ratio, max_sharpe_idxs = {}, {}\n",
    "\n",
    "# Loop through dates and get sharpe ratio for each portfolio\n",
    "for date in portfolio_returns.keys():\n",
    "    for i, ret in enumerate(portfolio_returns[date]):\n",
    "    \n",
    "        # Divide returns by the volatility for the date and index, i\n",
    "        sharpe_ratio.setdefault(date, []).append(ret / portfolio_volatility[date][i])\n",
    "\n",
    "    # Get the index of the best sharpe ratio for each date\n",
    "    max_sharpe_idxs[date] = np.argmax(sharpe_ratio[date])\n",
    "\n",
    "print(portfolio_returns[date][max_sharpe_idxs[date]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate EWMAs\n",
    "We will now work towards creating some features to be able to predict our ideal portfolios. We will simply use the price movement as a feature for now. To do this we will create a daily exponentially-weighted moving average (EWMA), then resample that to the monthly timeframe. Finally, we'll shift the monthly moving average of price one month in the future, so we can use it as a feature for predicting future portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate exponentially-weighted moving average of daily returns\n",
    "ewma_daily = returns_daily.ewm(span=30).mean()\n",
    "\n",
    "# Resample daily returns to first business day of the month with average for that month\n",
    "ewma_monthly = ewma_daily.resample('BMS').first()\n",
    "\n",
    "# Shift ewma for the month by 1 month forward so we can use it as a feature for future predictions \n",
    "ewma_monthly = ewma_monthly.shift(-1).dropna()\n",
    "\n",
    "print(ewma_monthly.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make features and targets\n",
    "To use machine learning to pick the best portfolio, we need to generate features and targets. Our features were just created in the last exercise – the exponentially weighted moving averages of prices. Our targets will be the best portfolios we found from the highest Sharpe ratio.\n",
    "\n",
    "We will use pandas' .iterrows() method to get the index, value pairs for the ewma_monthly DataFrame. We'll set the current value of ewma_monthly in the loop to be our features. Then we'll use the index of the best Sharpe ratio (from max_sharpe_idxs) to get the best portfolio_weights for each month and set that as a target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, features = [], []\n",
    "\n",
    "# Create features from price history and targets as ideal portfolio\n",
    "for date, ewma in ewma_monthly.iterrows():\n",
    "\n",
    "    # Get the index of the best sharpe ratio\n",
    "    best_idx = max_sharpe_idxs[date]\n",
    "    targets.append(portfolio_weights[date][best_idx])\n",
    "    features.append(ewma)  # add ewma to features\n",
    "\n",
    "targets = np.array(targets)\n",
    "features = np.array(features)\n",
    "print(targets[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot efficient frontier with best Sharpe ratio\n",
    "Let's now plot the efficient frontier again, but add a marker for the portfolio with the best Sharpe index. Visualizing our data is always a good idea to better understand it.\n",
    "\n",
    "Recall the efficient frontier is plotted in a scatter plot of portfolio volatility on the x-axis, and portfolio returns on the y-axis. We'll get the latest date we have in our data from covariances.keys(), although any of the portfolio_returns, etc, dictionaries could be used as well to get the date. Then we get volatilities and returns for the latest date we have from our portfolio_volatility and portfolio_returns. Finally we get the index of the portfolio with the best Sharpe index from max_sharpe_idxs[date], and plot everything with plt.scatter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most recent (current) returns and volatility\n",
    "date = sorted(covariances.keys())[-1]\n",
    "cur_returns = portfolio_returns[date]\n",
    "cur_volatility = portfolio_volatility[date]\n",
    "\n",
    "# Plot efficient frontier with sharpe as point\n",
    "plt.scatter(x=cur_volatility, y=cur_returns, alpha=0.1, color='blue')\n",
    "best_idx = max_sharpe_idxs[date]\n",
    "\n",
    "# Place an orange \"X\" on the point with the best Sharpe ratio\n",
    "plt.scatter(x=cur_volatility[best_idx], y=cur_returns[best_idx], marker='x', color='orange')\n",
    "plt.xlabel('Volatility')\n",
    "plt.ylabel('Returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions with a random forest\n",
    "In order to fit a machine learning model to predict ideal portfolios, we need to create train and test sets for evaluating performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train and test features\n",
    "train_size = int(0.85 * features.shape[0])\n",
    "train_features = features[:train_size]\n",
    "test_features = features[train_size:]\n",
    "train_targets = targets[:train_size]\n",
    "test_targets = targets[train_size:]\n",
    "\n",
    "# Fit the model and check scores on train and test\n",
    "rfr = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "rfr.fit(train_features, train_targets)\n",
    "print(rfr.score(train_features, train_targets))\n",
    "print(rfr.score(test_features, test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions and first evaluation\n",
    "Now that we have a trained random forest model (rfr), we want to use it to get predictions on the test set. We do this to evaluate our model's performance – at a basic level, is it doing as well or better than just buying the index, SPY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from model on train and test\n",
    "train_predictions = rfr.predict(train_features)\n",
    "test_predictions = rfr.predict(test_features)\n",
    "\n",
    "# Calculate and plot returns from our RF predictions and the SPY returns\n",
    "test_returns = np.sum(returns_monthly.iloc[train_size:] * test_predictions, axis=1)\n",
    "plt.plot(test_returns, label='algo')\n",
    "plt.plot(returns_monthly['SPY'].iloc[train_size:], label='SPY')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate returns\n",
    "Let's now see how our portfolio selection would perform as compared with just investing in the SPY. We'll do this to see if our predictions are promising, despite the low R2 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the effect of our portfolio selection on a hypothetical $1k investment\n",
    "cash = 1000\n",
    "algo_cash, spy_cash = [cash], [cash]  # set equal starting cash amounts\n",
    "for r in test_returns:\n",
    "    cash *= 1 + r\n",
    "    algo_cash.append(cash)\n",
    "\n",
    "# Calculate performance for SPY\n",
    "cash = 1000  # reset cash amount\n",
    "for r in returns_monthly['SPY'].iloc[train_size:]:\n",
    "    cash *= 1 + r\n",
    "    spy_cash.append(cash)\n",
    "\n",
    "print('algo returns:', (algo_cash[-1] - algo_cash[0]) / algo_cash[0])\n",
    "print('SPY returns:', (spy_cash[-1] - spy_cash[0]) / spy_cash[0])\n",
    "\n",
    "# Plot the algo_cash and spy_cash to compare overall returns\n",
    "plt.plot(algo_cash, label='algo')\n",
    "plt.plot(spy_cash, label='SPY')\n",
    "plt.legend()  # show the legend\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
