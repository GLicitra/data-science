{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link: https://www.kaggle.com/kinguistics/heartbeat-sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "import librosa as lr\n",
    "from glob import glob\n",
    "\n",
    "# List all the wav files in the folder\n",
    "audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "# Read in the first audio file, create the time array\n",
    "audio, sfreq = lr.load(audio_files[0])\n",
    "time = np.arange(0, len(audio)) / sfreq\n",
    "\n",
    "# Plot audio over time\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(time, audio)\n",
    "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several seconds of heartbeat sounds in here, though note that most of this time is silence. A common procedure in machine learning is to separate the datapoints with lots of stuff happening from the ones that don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Using raw timeseries data is too noisy for classification\n",
    "- We need to calculate features!\n",
    "- As easy start: summarize the audio data\n",
    "\n",
    "A common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_and_make_titles():\n",
    "   axs[0, 0].set(title=\"Normal Heartbeats\")\n",
    "   axs[0, 1].set(title=\"Abnormal Heartbeats\")\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
    "\n",
    "# Calculate the time array\n",
    "time = np.arange(normal.shape[0]) / sfreq\n",
    "\n",
    "# Stack the normal/abnormal audio so you can loop and plot\n",
    "stacked_audio = np.hstack([normal, abnormal]).T\n",
    "\n",
    "# Loop through each audio file / ax object and plot\n",
    "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
    "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
    "    ax.plot(time, iaudio)\n",
    "show_plot_and_make_titles()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across the audio files of each DataFrame\n",
    "mean_normal = np.mean(normal,     axis=1)\n",
    "mean_abnormal = np.mean(abnormal, axis=1)\n",
    "\n",
    "# Plot each average over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
    "ax1.plot(mean_normal.index, mean_normal.values)\n",
    "ax1.set(title=\"Normal Data\")\n",
    "ax2.plot(mean_abnormal.index, mean_abnormal.values)\n",
    "ax2.set(title=\"Abnormal Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a classification model\n",
    "While eye-balling differences is a useful way to gain an intuition for the data, let's see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data.\n",
    "\n",
    "We've split the two DataFrames (normal and abnormal) into X_train, X_test, y_train, and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and score them manually\n",
    "predictions = model.predict(X_test)\n",
    "print(sum(predictions == y_test.squeeze()) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your predictions didn't do so well. That's because the features you're using as inputs to the model (raw data) aren't very good at differentiating classes. Next, you'll explore how to calculate some more complex features that may improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute envelope sound\n",
    "# Plot the raw data first\n",
    "audio.plot(figsize=(10, 5))\n",
    "plt.show()\n",
    "\n",
    "# Rectify the audio signal\n",
    "audio_rectified = audio.apply(np.abs)\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified.plot(figsize=(10, 5))\n",
    "plt.show()\n",
    "\n",
    "# Smooth by applying a rolling mean\n",
    "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
    "\n",
    "# Plot the result\n",
    "audio_rectified_smooth.plot(figsize=(10, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier with cleaner features\n",
    "# Fit the model and score on testing data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))\n",
    "# expected accuracy 71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative features: The tempogram\n",
    "One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you'll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more.\n",
    "Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we'll access our Pandas data as a Numpy array with the .values attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tempo of the sounds\n",
    "tempos = []\n",
    "for col, i_audio in audio.items():\n",
    "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
    "\n",
    "# Convert the list to an array so you can manipulate it more easily\n",
    "tempos = np.array(tempos)\n",
    "\n",
    "# Calculate statistics of each tempo\n",
    "tempos_mean = tempos.mean(axis=-1)\n",
    "tempos_std = tempos.std(axis=-1)\n",
    "tempos_max = tempos.max(axis=-1)\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected accuracy: 56% \n",
    "Note that your predictive power may not have gone up (because this dataset is quite small), but you now have a more rich feature representation of audio that your model can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the stft function\n",
    "from librosa.core import stft\n",
    "\n",
    "# Prepare the STFT\n",
    "HOP_LENGTH = 2**4\n",
    "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)\n",
    "\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Convert into decibels\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Compare the raw audio to the spectrogram of the audio\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "axs[0].plot(time, audio)\n",
    "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lr\n",
    "from librosa.core import amplitude_to_db\n",
    "from librosa.display import specshow\n",
    "\n",
    "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
    "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
    "centroids = lr.feature.spectral_centroid(S=spec)[0]\n",
    "\n",
    "# Convert spectrogram to decibels for visualization\n",
    "spec_db = amplitude_to_db(spec)\n",
    "\n",
    "# Display these features on top of the spectrogram\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH)\n",
    "ax.plot(times_spec, centroids)\n",
    "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
    "ax.set(ylim=[None, 6000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the spectral centroid and bandwidth characterize the spectral content in each sound over time. They give us a summary of the spectral content that we can use in a classifier.\n",
    "\n",
    "### Combining many features in a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each spectrogram\n",
    "bandwidths = []\n",
    "centroids = []\n",
    "\n",
    "for spec in spectrograms:\n",
    "    # Calculate the mean spectral bandwidth\n",
    "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
    "    # Calculate the mean spectral centroid\n",
    "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
    "    # Collect the values\n",
    "    bandwidths.append(this_mean_bandwidth)  \n",
    "    centroids.append(this_mean_centroid)\n",
    "\n",
    "# Create X and y arrays\n",
    "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
    "y = labels.reshape([-1, 1])\n",
    "\n",
    "# Fit the model and score on testing data\n",
    "percent_score = cross_val_score(model, X, y, cv=5)\n",
    "print(np.mean(percent_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You calculated many different features of the audio, and combined each of them under the assumption that they provide independent information that can be used in classification. You may have noticed that the accuracy of your models varied a lot when using different set of features. This chapter was focused on creating new \"features\" from raw data and not obtaining the best accuracy. To improve the accuracy, you want to find the right features that provide relevant information and also build models on much larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
